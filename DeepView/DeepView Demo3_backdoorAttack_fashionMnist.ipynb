{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from deepview import DeepView\n",
    "import demo_utils as demo\n",
    "from demo_utils import Net\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Fashion MNIST data set and train a simple ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device will be detected automatically\n",
    "# Set to 'cpu' or 'cuda:0' to set the device manually\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "trainloader, testset, testloader = demo.make_FashionMNIST_dataset()\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')\n",
    "\n",
    "dim_img   = 28\n",
    "dim_sq    = dim_img*dim_img\n",
    "col_ch    = 1\n",
    "n_classes = len(classes)\n",
    "\n",
    "# init the model\n",
    "torch.manual_seed(42)\n",
    "torch_model = Net().to(device)\n",
    "optimizer = optim.Adam(torch_model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the model on data including backdoors\n",
    "# testing on clean test set\n",
    "\n",
    "n_backd = 600 * 8\n",
    "# backdoor 'bag' as 'trousers'\n",
    "backd_a = 8 # attacked class\n",
    "backd_t = 1 # target class\n",
    "\n",
    "for epoch in range(1, 10 + 1):\n",
    "    demo.train_backdoor(torch_model, device, trainloader, optimizer, epoch, backd_a=backd_a, backd_t=backd_t, n_backd=n_backd)\n",
    "    #train(model, device, trainloader, optimizer, epoch)\n",
    "    demo.test(torch_model, device, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select a  data subset and add backdoors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the test data\n",
    "show_backdoors = 1\n",
    "\n",
    "n_data  = 600 # dataset.__len__()\n",
    "# pick random instances\n",
    "np.random.seed(42)\n",
    "rand_posis = np.random.permutation(len(testset))[0:n_data]\n",
    "\n",
    "# check how many instances are from our attacked class\n",
    "n_attackable = 0\n",
    "for i in range(0, n_data):\n",
    "    # load the data\n",
    "    data      = testset.__getitem__(rand_posis[i])\n",
    "    if data[1] == backd_a:\n",
    "        n_attackable +=1 \n",
    "        \n",
    "n_att = 20\n",
    "print('#points from target class:', n_attackable, ', #attacking points', n_att)\n",
    "att_idx = np.zeros(n_att, dtype=int)\n",
    "\n",
    "# load the data and add backdoors\n",
    "#X    = torch.zeros([n_data+add_points, col_ch*dim_sq]).to(device)\n",
    "X         = np.empty([n_data, col_ch, dim_img, dim_img])\n",
    "labs      = np.empty([n_data], dtype=int)\n",
    "pred_labs = np.empty([n_data], dtype=int)\n",
    "\n",
    "\n",
    "if show_backdoors:\n",
    "    print(\"Displaying backdoored points with backdoor label and predicted label\")\n",
    "    fig, axes = plt.subplots(4, round(n_att/4), figsize=(12, 8))\n",
    "\n",
    "attacked = 0\n",
    "for i in range(0, n_data):\n",
    "    # load the data\n",
    "    data      = testset.__getitem__(rand_posis[i])\n",
    "\n",
    "    data_item = torch.zeros([1, col_ch, dim_img, dim_img]).to(device)\n",
    "    data_item[:,:,:,:] = data[0]\n",
    "    labs[i]   = data[1]\n",
    "    \n",
    "    # attack the first n_att images from attacked class\n",
    "    if (attacked < n_att) & (labs[i].item() == backd_a):\n",
    "        labs[i] = backd_t\n",
    "        demo.add_backdoor(data_item[0])\n",
    "        att_idx[attacked] = i\n",
    "        attacked += 1\n",
    "    \n",
    "    output                  = torch_model(data_item)\n",
    "    pred_labs[i]            = output.detach()[0].argmax().item()\n",
    "    \n",
    "    if (data[1] == backd_a) & (labs[i].item() == backd_t) & show_backdoors:\n",
    "        if attacked-1 < round(n_att/4):\n",
    "            curr_col = attacked-1\n",
    "            cur_row  = 0\n",
    "        elif attacked-1 < 2*round(n_att/4):\n",
    "            curr_col = attacked-1 - round(n_att/4)\n",
    "            cur_row  = 1\n",
    "        elif attacked-1 < 3*round(n_att/4):\n",
    "            curr_col = attacked-1 - 2*round(n_att/4)\n",
    "            cur_row  = 2\n",
    "        elif attacked-1 < 4*round(n_att/4):\n",
    "            curr_col = attacked-1 - 3*round(n_att/4)\n",
    "            cur_row  = 3\n",
    "        \n",
    "        axes[cur_row, curr_col].imshow(data_item[0,0].detach().cpu().numpy(), cmap='gray')\n",
    "        axes[cur_row, curr_col].axis('off')\n",
    "        axes[cur_row, curr_col].set_title(classes[labs[i].item()] + \", \" + classes[output.detach()[0].argmax().item()])\n",
    "    \n",
    "    X[i,:,:,:] = data_item[0,:,:,:].detach().cpu().numpy()\n",
    "    # first, load the data and add their index in the last dim\n",
    "    #X[i,0:-1] = torch.reshape(data_item.detach(), [col_ch*dim_sq])\n",
    "    #X[i,-1]   = i\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize and apply DeepView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(image, point, prediction, label):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image.squeeze())\n",
    "    pred = classes[prediction]\n",
    "    if label is None:\n",
    "        ax.set_title('pred: %s' % pred)\n",
    "    else:\n",
    "        label = classes[label]\n",
    "        ax.set_title('pred: %s - label: %s' % (pred, label))\n",
    "    fig.show()\n",
    "\n",
    "# --- Deep View Parameters ----\n",
    "batch_size = 1024\n",
    "max_samples = 500\n",
    "data_shape = (col_ch, dim_img, dim_img)\n",
    "n = 5\n",
    "lam = .65\n",
    "resolution = 100\n",
    "cmap = 'tab10'\n",
    "title = 'ConvNet - FashionMnist with backdoors'\n",
    "\n",
    "deepview = DeepView(torch_model.predict_numpy, classes, max_samples, batch_size, data_shape, \n",
    "                    n, lam, resolution, cmap, title=title, data_viz=visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepview.evaluate import evaluate\n",
    "# run deepview\n",
    "\n",
    "umapParms = {\n",
    "    \"random_state\": 42*42,\n",
    "    \"n_neighbors\": 30,\n",
    "    \"min_dist\" : 1,\n",
    "    \"verbose\" : True\n",
    "} \n",
    "deepview._init_mappers(None, None, umapParms)\n",
    "#deepview.resolution = 200 # uncomment to increase resolution\n",
    "# TODO: a = 400\n",
    "\n",
    "t0 = time.time()\n",
    "# create a visualization\n",
    "deepview.add_samples(X, labs)\n",
    "\n",
    "#deepview.update_mappings()\n",
    "\n",
    "deepview.show()\n",
    "\n",
    "print('Time to calculate visualization for %d samples: %.2f sec' % (n_data, time.time() - t0))\n",
    "\n",
    "# calculate the quality of the projection (pi)\n",
    "print('Evaluation of DeepView: %s\\n' % deepview.title)\n",
    "evaluate(deepview, deepview.samples, deepview.y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
