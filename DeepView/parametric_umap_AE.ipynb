{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric umap autoencoder to visualize GAP layer data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load GAP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from tensorboardX import SummaryWriter\n",
    "from cifar10_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = 2048\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Model successfully...\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = resnet50(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(\"Load Model successfully...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "data_shape = (2048,)\n",
    "\n",
    "ADD_NOISE = False\n",
    "n_hidden = 500\n",
    "dim_img = INPUT_SIZE  # number of pixels for a MNIST image\n",
    "dim_z = 2\n",
    "\n",
    "# train\n",
    "n_epochs = 500\n",
    "batch_size = 200\n",
    "learn_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 2048)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CIFAR10 Test dataset and dataloader declaration\n",
    "CIFAR_NORM = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(*CIFAR_NORM)])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=2000,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "training_data = np.zeros((50000, 3, 32, 32))\n",
    "for i, (data, target) in enumerate(trainloader, 0):\n",
    "    r1, r2 = i * 2000, (i + 1) * 2000\n",
    "    training_data[r1:r2] = data\n",
    "\n",
    "raw_input_X = torch.from_numpy(training_data).to(device, dtype=torch.float)\n",
    "input_X = np.zeros([len(raw_input_X), data_shape[0]])\n",
    "n_batches = max(math.ceil(len(raw_input_X) / batch_size), 1)\n",
    "for b in range(n_batches):\n",
    "    r1, r2 = b * batch_size, (b + 1) * batch_size\n",
    "    inputs = raw_input_X[r1:r2]\n",
    "    with torch.no_grad():\n",
    "        pred = model.gap(inputs).cpu().numpy()\n",
    "        input_X[r1:r2] = pred\n",
    "train_data = input_X    # (50000,2048)\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 2048)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset = torchvision.datasets.CIFAR10(root='data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=2000,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testing_data = np.zeros((10000, 3, 32, 32))\n",
    "for i, (data, target) in enumerate(testloader, 0):\n",
    "    r1, r2 = i * 2000, (i + 1) * 2000\n",
    "    testing_data[r1:r2] = data\n",
    "\n",
    "raw_input_X = torch.from_numpy(testing_data).to(device, dtype=torch.float)\n",
    "input_X = np.zeros([len(raw_input_X), data_shape[0]])\n",
    "n_batches = max(math.ceil(len(raw_input_X) / batch_size), 1)\n",
    "for b in range(n_batches):\n",
    "    r1, r2 = b * batch_size, (b + 1) * batch_size\n",
    "    inputs = raw_input_X[r1:r2]\n",
    "    with torch.no_grad():\n",
    "        pred = model.gap(inputs).cpu().numpy()\n",
    "        input_X[r1:r2] = pred\n",
    "test_data = input_X    # (10000,2048)\n",
    "\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load parametric umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap.parametric_umap import ParametricUMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 3,017,474\n",
      "Trainable params: 3,017,474\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define encoder\n",
    "import tensorflow as tf\n",
    "dims = (2048,)\n",
    "n_components = 2\n",
    "encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=dims),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=1024, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=256, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=n_components),\n",
    "])\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 256)               768       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "recon (Dense)                (None, 2048)              2099200   \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 2048)              0         \n",
      "=================================================================\n",
      "Total params: 3,019,520\n",
      "Trainable params: 3,019,520\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define the decoder\n",
    "decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(n_components)),\n",
    "    tf.keras.layers.Dense(units=256, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=1024, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=np.product(dims), name=\"recon\", activation=None),\n",
    "    tf.keras.layers.Reshape(dims),\n",
    "\n",
    "])\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2048)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "batch_size = 1000\n",
    "# validation_data = train_data[-10000:]\n",
    "train_data = train_data\n",
    "\n",
    "keras_fit_kwargs = {\"callbacks\": [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='loss',\n",
    "        min_delta=10**-2,\n",
    "        patience=10,\n",
    "        verbose=1,\n",
    "    )\n",
    "]}\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = ParametricUMAP(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    dims=dims,\n",
    "    parametric_reconstruction=True,\n",
    "#     reconstruction_validation=validation_data,\n",
    "    autoencoder_loss=True,\n",
    "    n_training_epochs=100,\n",
    "    keras_fit_kwargs = keras_fit_kwargs,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParametricUMAP(autoencoder_loss=True,\n",
      "               decoder=<tensorflow.python.keras.engine.sequential.Sequential object at 0x000001F3E5C6A3C8>,\n",
      "               dims=(2048,),\n",
      "               encoder=<tensorflow.python.keras.engine.sequential.Sequential object at 0x000001F3E5C1F320>,\n",
      "               keras_fit_kwargs={'callbacks': [<tensorflow.python.keras.callbacks.EarlyStopping object at 0x000001F3E5C488D0>]},\n",
      "               n_training_epochs=100,\n",
      "               optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x000001F3E5C48D30>,\n",
      "               parametric_reconstruction=True)\n",
      "Construct fuzzy simplicial set\n",
      "Mon Nov 30 11:44:42 2020 Finding Nearest Neighbors\n",
      "Mon Nov 30 11:44:42 2020 Building RP forest with 16 trees\n",
      "Mon Nov 30 11:44:45 2020 NN descent for 16 iterations\n",
      "\t 1  /  16\n",
      "\t 2  /  16\n",
      "\t 3  /  16\n",
      "\tStopping threshold met -- exiting after 3 iterations\n",
      "Mon Nov 30 11:44:58 2020 Finished Nearest Neighbor Search\n",
      "Mon Nov 30 11:45:00 2020 Construct embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Downloads\\anaconda\\envs\\DR2\\lib\\site-packages\\umap\\parametric_umap.py:270: UserWarning: Data should be scaled to the range 0-1 for cross-entropy reconstruction loss.\n",
      "  \"Data should be scaled to the range 0-1 for cross-entropy reconstruction loss.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "644/644 [==============================] - 83s 127ms/step - loss: 0.3369 - reconstruction_loss: 0.1553 - umap_loss: 0.1817\n",
      "Epoch 2/1000\n",
      "644/644 [==============================] - 82s 127ms/step - loss: 0.1918 - reconstruction_loss: 0.0971 - umap_loss: 0.0947\n",
      "Epoch 3/1000\n",
      "644/644 [==============================] - 85s 131ms/step - loss: 0.1849 - reconstruction_loss: 0.0955 - umap_loss: 0.0894\n",
      "Epoch 4/1000\n",
      "644/644 [==============================] - 85s 132ms/step - loss: 0.1812 - reconstruction_loss: 0.0942 - umap_loss: 0.0870\n",
      "Epoch 5/1000\n",
      "644/644 [==============================] - 85s 133ms/step - loss: 0.1808 - reconstruction_loss: 0.0939 - umap_loss: 0.0868\n",
      "Epoch 6/1000\n",
      "644/644 [==============================] - 85s 132ms/step - loss: 0.1794 - reconstruction_loss: 0.0937 - umap_loss: 0.0857\n",
      "Epoch 7/1000\n",
      "644/644 [==============================] - 86s 133ms/step - loss: 0.1788 - reconstruction_loss: 0.0937 - umap_loss: 0.0851\n",
      "Epoch 8/1000\n",
      "644/644 [==============================] - 86s 133ms/step - loss: 0.1776 - reconstruction_loss: 0.0936 - umap_loss: 0.0841\n",
      "Epoch 9/1000\n",
      "644/644 [==============================] - 85s 132ms/step - loss: 0.1777 - reconstruction_loss: 0.0938 - umap_loss: 0.0839\n",
      "Epoch 10/1000\n",
      "644/644 [==============================] - 85s 132ms/step - loss: 0.1765 - reconstruction_loss: 0.0937 - umap_loss: 0.0828\n",
      "Epoch 11/1000\n",
      "644/644 [==============================] - 85s 132ms/step - loss: 0.1757 - reconstruction_loss: 0.0935 - umap_loss: 0.0822\n",
      "Epoch 12/1000\n",
      "644/644 [==============================] - 84s 131ms/step - loss: 0.1767 - reconstruction_loss: 0.0937 - umap_loss: 0.0830\n",
      "Epoch 13/1000\n",
      "644/644 [==============================] - 85s 132ms/step - loss: 0.1780 - reconstruction_loss: 0.0937 - umap_loss: 0.0843\n",
      "Epoch 14/1000\n",
      "644/644 [==============================] - 83s 130ms/step - loss: 0.1763 - reconstruction_loss: 0.0936 - umap_loss: 0.0827\n",
      "Epoch 15/1000\n",
      "644/644 [==============================] - 84s 131ms/step - loss: 0.1753 - reconstruction_loss: 0.0935 - umap_loss: 0.0818\n",
      "Epoch 16/1000\n",
      "644/644 [==============================] - 85s 132ms/step - loss: 0.1751 - reconstruction_loss: 0.0934 - umap_loss: 0.0817\n",
      "Epoch 00016: early stopping\n",
      "1563/1563 [==============================] - 3s 2ms/step\n",
      "Mon Nov 30 12:08:44 2020 Finished embedding\n"
     ]
    }
   ],
   "source": [
    "embedding = embedder.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: ./parametric_umap_models/parametric_umap_autoencoder\\encoder\\assets\n",
      "Keras encoder model saved to ./parametric_umap_models/parametric_umap_autoencoder\\encoder\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: ./parametric_umap_models/parametric_umap_autoencoder\\decoder\\assets\n",
      "Keras decoder model saved to ./parametric_umap_models/parametric_umap_autoencoder\\decoder\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: ./parametric_umap_models/parametric_umap_autoencoder\\parametric_model\\assets\n",
      "Keras full model saved to ./parametric_umap_models/parametric_umap_autoencoder\\parametric_model\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'ParametricUMAP._define_model.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-68635b87329b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membedder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./parametric_umap_models/parametric_umap_autoencoder'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Downloads\\anaconda\\envs\\DR2\\lib\\site-packages\\umap\\parametric_umap.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, save_location, verbose)\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0mmodel_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"model.pkl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m                 \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pickle of ParametricUMAP model saved to {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Downloads\\anaconda\\envs\\DR2\\lib\\site-packages\\umap\\parametric_umap.py\u001b[0m in \u001b[0;36m__getstate__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[1;31m# this function supports pickling, making sure that objects can be pickled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mshould_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Downloads\\anaconda\\envs\\DR2\\lib\\site-packages\\umap\\parametric_umap.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[1;31m# this function supports pickling, making sure that objects can be pickled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mshould_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Downloads\\anaconda\\envs\\DR2\\lib\\site-packages\\umap\\parametric_umap.py\u001b[0m in \u001b[0;36mshould_pickle\u001b[1;34m(key, val)\u001b[0m\n\u001b[0;32m    863\u001b[0m         \u001b[1;31m## make sure object can be pickled and then re-read\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         \u001b[1;31m# pickle object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 865\u001b[1;33m         \u001b[0mpickled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"base64\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    866\u001b[0m         \u001b[1;31m# unpickle object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m         \u001b[0munpickled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"base64\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't pickle local object 'ParametricUMAP._define_model.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "# embedder.save('./parametric_umap_models/parametric_umap_autoencoder',verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval local structure preserving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trustworthiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 2048), (10000, 2048))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import trustworthiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 1s 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9848552585301443"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embedded = embedder.transform(train_data)\n",
    "t = trustworthiness(train_data, train_embedded, n_neighbors=10, metric='euclidean')\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 8s 43ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9828336862136311"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embedded = embedder.transform(test_data)\n",
    "embedded_t = trustworthiness(test_data, test_embedded, n_neighbors=10, metric='euclidean')\n",
    "embedded_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 1s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "total_data = np.concatenate((train_data, test_data),axis=0)\n",
    "total_embedded = embedder.transform(total_data)\n",
    "total_t = trustworthiness(total_data, total_embedded, n_neighbors=10, metric='euclidean')\n",
    "total_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Project data\n",
    "z = embedder.transform(train_data)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=1, figsize=(10, 8))\n",
    "sc = ax.scatter(\n",
    "    z[:, 0],\n",
    "    z[:, 1],\n",
    "#     c=train_labels,\n",
    "    cmap=\"tab10\",\n",
    "    s=0.1,\n",
    "    alpha=0.5,\n",
    "    rasterized=True,\n",
    ")\n",
    "ax.axis('equal')\n",
    "ax.set_title(\"parametric UMAP autoencoder embeddings-training data\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project data\n",
    "z = embedder.transform(test_data)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=1, figsize=(10, 8))\n",
    "sc = ax.scatter(\n",
    "    z[:, 0],\n",
    "    z[:, 1],\n",
    "#     c=train_labels,\n",
    "    cmap=\"tab10\",\n",
    "    s=0.1,\n",
    "    alpha=0.5,\n",
    "    rasterized=True,\n",
    ")\n",
    "ax.axis('equal')\n",
    "ax.set_title(\"parametric UMAP autoencoder embeddings-testing data\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project data\n",
    "z = embedder.transform(np.concatenate((train_data, test_data),axis=0))\n",
    "\n",
    "fig, ax = plt.subplots(ncols=1, figsize=(10, 8))\n",
    "sc = ax.scatter(\n",
    "    z[:, 0],\n",
    "    z[:, 1],\n",
    "#     c=train_labels,\n",
    "    cmap=\"tab10\",\n",
    "    s=0.1,\n",
    "    alpha=0.5,\n",
    "    rasterized=True,\n",
    ")\n",
    "ax.axis('equal')\n",
    "ax.set_title(\"parametric UMAP autoencoder embeddings-total data\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_num = 50000\n",
    "adv_succ_num = 4000 # max \n",
    "adv_fail_num = 4000 # max\n",
    "true_num = 10000\n",
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "def pred_wrapper(x):\n",
    "    with torch.no_grad():\n",
    "        # tensor = torch.from_numpy(x).to(device, dtype=torch.float)\n",
    "        logits = model.fc(x)\n",
    "        probabilities = softmax(logits).cpu().numpy()\n",
    "    return probabilities\n",
    "\n",
    "# ---------------------load data------------------------------\n",
    "test_samples = training_data[:test_num]\n",
    "true_samples = testing_data[:true_num]\n",
    "\n",
    "Adv_succ = np.load(\"adv_testset\\\\succ.npy\")\n",
    "Adv_fail = np.load(\"adv_testset\\\\fail.npy\")\n",
    "\n",
    "adv_succ = torch.from_numpy(Adv_succ[-adv_succ_num:]).to(device, dtype=torch.float)\n",
    "adv_fail = torch.from_numpy(Adv_fail[-adv_fail_num:]).to(device, dtype=torch.float)\n",
    "\n",
    "raw_input_X = torch.from_numpy(test_samples).to(device, dtype=torch.float)\n",
    "raw_input_X = torch.cat((raw_input_X, adv_succ), axis=0)\n",
    "raw_input_X = torch.cat((raw_input_X, adv_fail), axis=0)\n",
    "raw_input_X = torch.cat((raw_input_X, true_samples), axis=0)\n",
    "\n",
    "input_X = np.zeros([len(raw_input_X), data_shape[0]])\n",
    "output_Y = np.zeros(len(raw_input_X))\n",
    "n_batches = max(math.ceil(len(raw_input_X) / batch_size), 1)\n",
    "for b in range(n_batches):\n",
    "    r1, r2 = b * batch_size, (b + 1) * batch_size\n",
    "    inputs = raw_input_X[r1:r2]\n",
    "    with torch.no_grad():\n",
    "        pred = model.gap(inputs).cpu().numpy()\n",
    "        input_X[r1:r2] = pred\n",
    "        pred = pred_wrapper(torch.from_numpy(pred).to(device, dtype=torch.float)).argmax(axis=1)\n",
    "        output_Y[r1:r2] = pred\n",
    "\n",
    "embedded = embedder.transform(input_X)\n",
    "y_PRR = embedder.inverse_transform(embedded)\n",
    "# y_PRR, embedded = ae.get_ae(encoder, decoder, torch.from_numpy(input_X).float().to(device), batch_size)\n",
    "# y_PRR = y_PRR.detach().numpy()\n",
    "\n",
    "# pick samples for training and testing\n",
    "train_samples = input_X[:test_num]\n",
    "train_labels = output_Y[:test_num]\n",
    "test_adv_succ = input_X[test_num:test_num + adv_succ_num]\n",
    "test_adv_succ_labels = output_Y[test_num:test_num + adv_succ_num]\n",
    "test_adv_fail = input_X[test_num + adv_succ_num:test_num + adv_succ_num + adv_fail_num]\n",
    "test_adv_fail_labels = output_Y[test_num + adv_succ_num:test_num + adv_succ_num + adv_fail_num]\n",
    "test_true = input_X[-true_num:]\n",
    "test_true_labels = output_Y[-true_num:]\n",
    "# apply inverse mapping to embedded samples and\n",
    "# predict the reconstructions\n",
    "train_recon = y_PRR[:test_num]\n",
    "train_recon_preds = pred_wrapper(torch.from_numpy(train_recon).to(device, dtype=torch.float)).argmax(axis=1)\n",
    "train_confidence = pred_wrapper(torch.from_numpy(train_samples).to(device, dtype=torch.float)).max(axis=1)\n",
    "train_new_confidence = pred_wrapper(torch.from_numpy(train_recon).to(device, dtype=torch.float)).max(axis=1)\n",
    "\n",
    "adv_succ_recon = y_PRR[test_num:test_num + adv_succ_num]\n",
    "adv_succ_recon_preds = pred_wrapper(torch.from_numpy(adv_succ_recon).to(device, dtype=torch.float)).argmax(\n",
    "    axis=1)\n",
    "adv_succ_confidence = pred_wrapper(torch.from_numpy(test_adv_succ).to(device, dtype=torch.float)).max(\n",
    "    axis=1)\n",
    "adv_succ_new_confidence = pred_wrapper(torch.from_numpy(adv_succ_recon).to(device, dtype=torch.float)).max(\n",
    "    axis=1)\n",
    "\n",
    "adv_fail_recon = y_PRR[test_num + adv_succ_num:test_num + adv_succ_num + adv_fail_num]\n",
    "adv_fail_recon_preds = pred_wrapper(torch.from_numpy(adv_fail_recon).to(device, dtype=torch.float)).argmax(\n",
    "    axis=1)\n",
    "adv_fail_confidence = pred_wrapper(torch.from_numpy(test_adv_fail).to(device, dtype=torch.float)).max(\n",
    "    axis=1)\n",
    "adv_fail_new_confidence = pred_wrapper(torch.from_numpy(adv_fail_recon).to(device, dtype=torch.float)).max(\n",
    "    axis=1)\n",
    "\n",
    "true_recon = y_PRR[-true_num:]\n",
    "true_recon_preds = pred_wrapper(torch.from_numpy(true_recon).to(device, dtype=torch.float)).argmax(axis=1)\n",
    "true_confidence = pred_wrapper(torch.from_numpy(test_true).to(device, dtype=torch.float)).max(axis=1)\n",
    "true_new_confidence = pred_wrapper(torch.from_numpy(true_recon).to(device, dtype=torch.float)).max(axis=1)\n",
    "\n",
    "# calculate pred accuracy\n",
    "n_correct = np.sum(train_labels == train_recon_preds)\n",
    "train_acc = 100 * n_correct / test_num\n",
    "\n",
    "n_correct = np.sum(test_adv_succ_labels == adv_succ_recon_preds)\n",
    "test_adv_succ_acc = 100 * n_correct / float(adv_succ_num)\n",
    "\n",
    "n_correct = np.sum(test_adv_fail_labels == adv_fail_recon_preds)\n",
    "test_adv_fail_acc = 100 * n_correct / float(adv_fail_num)\n",
    "\n",
    "n_correct = np.sum(test_true_labels == true_recon_preds)\n",
    "test_true_acc = 100 * n_correct / float(true_num)\n",
    "\n",
    "# calculate distance\n",
    "train_distance = np.mean(np.sqrt(np.sum(np.square(train_samples - train_recon), axis=1)))\n",
    "adv_succ_distance = np.mean(np.sqrt(np.sum(np.square(test_adv_succ - adv_succ_recon), axis=1)))\n",
    "adv_fail_distance = np.mean(np.sqrt(np.sum(np.square(test_adv_fail - adv_fail_recon), axis=1)))\n",
    "true_distance = np.mean(np.sqrt(np.sum(np.square(test_true - true_recon), axis=1)))\n",
    "\n",
    "# calculate confidence difference\n",
    "train_conf_diff = np.mean(np.abs(train_confidence - train_new_confidence))\n",
    "adv_succ_conf_diff = np.mean(np.abs(adv_succ_confidence - adv_succ_new_confidence))\n",
    "adv_fail_conf_diff = np.mean(np.abs(adv_fail_confidence - adv_fail_new_confidence))\n",
    "true_conf_diff = np.mean(np.abs(true_confidence - true_new_confidence))\n",
    "\n",
    "print(\"train acc:{:.2f}%, test_succ acc:{:.2f}%, test_fail acc:{:.2f}%, test_true acc:{:.2f}%\".format(\n",
    "    train_acc,\n",
    "    test_adv_succ_acc,\n",
    "    test_adv_fail_acc,\n",
    "    test_true_acc))\n",
    "\n",
    "print(\"train distance:{:.2f}, test_succ dis:{:.2f}, test_fail dis:{:.2f}, test_true dis:{:.2f}\".format(\n",
    "    train_distance,\n",
    "    adv_succ_distance,\n",
    "    adv_fail_distance,\n",
    "    true_distance))\n",
    "print(\n",
    "    \"train con diff:{:.2f}, test_succ con dif:{:.2f}, test_fail con diff:{:.2f}, test_true con diff:{:.2f}\".format(\n",
    "        train_conf_diff,\n",
    "        adv_succ_conf_diff,\n",
    "        adv_fail_conf_diff,\n",
    "        true_conf_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2.24464457e-05, 1.18806839e-01, 1.91824138e-02, ...,\n",
       "         7.12189749e-02, 0.00000000e+00, 5.84107824e-04],\n",
       "        [4.93609114e-04, 6.57476438e-03, 7.72310561e-03, ...,\n",
       "         1.51221373e-03, 0.00000000e+00, 1.09400717e-05],\n",
       "        [1.18356664e-02, 5.96838910e-03, 3.03422183e-01, ...,\n",
       "         4.49039554e-03, 3.07281408e-03, 1.70014165e-02],\n",
       "        ...,\n",
       "        [3.79591226e-03, 2.61280060e-01, 1.68406207e-03, ...,\n",
       "         5.30657433e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "        [9.02255252e-03, 5.92675619e-03, 3.36675823e-01, ...,\n",
       "         1.21193682e-03, 3.69105255e-03, 1.50678279e-02],\n",
       "        [0.00000000e+00, 9.76955444e-02, 0.00000000e+00, ...,\n",
       "         4.82328273e-02, 0.00000000e+00, 5.50889410e-02]]),\n",
       " array([[-5.9763203, -2.5881884, -5.224879 , ..., -2.5188477, -4.730977 ,\n",
       "         -5.0103736],\n",
       "        [-6.8242946, -5.548279 , -7.0154877, ..., -6.782976 , -6.996356 ,\n",
       "         -7.970709 ],\n",
       "        [-4.75567  , -5.3914866, -0.8326197, ..., -5.792511 , -4.7953076,\n",
       "         -4.0542755],\n",
       "        ...,\n",
       "        [-6.4172707, -0.8665272, -6.078951 , ..., -2.7482605, -5.657724 ,\n",
       "         -5.09402  ],\n",
       "        [-4.827131 , -5.5655255, -0.7911216, ..., -5.9795427, -4.919854 ,\n",
       "         -4.1672215],\n",
       "        [-7.334951 , -2.831545 , -5.275039 , ..., -3.688819 , -7.739397 ,\n",
       "         -3.9230673]], dtype=float32),\n",
       " array([0.90377975, 0.98292786, 0.96952844, ..., 0.9024353 , 0.9847586 ,\n",
       "        0.8525023 ], dtype=float32),\n",
       " array([0.9999982, 1.       , 1.       , ..., 1.       , 1.       ,\n",
       "        1.       ], dtype=float32))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_confidence = pred_wrapper(torch.from_numpy(test_true).to(device, dtype=torch.float)).max(axis=1)\n",
    "true_new_confidence = pred_wrapper(torch.from_numpy(true_recon).to(device, dtype=torch.float)).max(axis=1)\n",
    "test_true,true_recon,true_confidence, true_new_confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.23149721324443817, 0.19227854907512665, 0.18922977149486542, 0.18519651889801025, 0.18418648838996887, 0.18199673295021057, 0.18077556788921356, 0.18097063899040222, 0.18086418509483337, 0.1812390387058258, 0.18028536438941956, 0.1795811504125595, 0.17900389432907104, 0.17854735255241394, 0.17918893694877625, 0.17731943726539612], 'reconstruction_loss': [0.10992393642663956, 0.09523361176252365, 0.0958896055817604, 0.09429077059030533, 0.09416545927524567, 0.09398489445447922, 0.0938488319516182, 0.09391828626394272, 0.09372079372406006, 0.09435935318470001, 0.0937972217798233, 0.09366452693939209, 0.09362693876028061, 0.09360148757696152, 0.09357178956270218, 0.09343650937080383], 'umap_loss': [0.12157342582941055, 0.09704484045505524, 0.09334011375904083, 0.09090562164783478, 0.09002089500427246, 0.08801175653934479, 0.08692678809165955, 0.08705238252878189, 0.08714330196380615, 0.08687962591648102, 0.08648820966482162, 0.08591663837432861, 0.08537700027227402, 0.0849459320306778, 0.08561716228723526, 0.08388294279575348]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting loss\n",
    "import matplotlib.pyplot as plt\n",
    "print(embedder._history)\n",
    "\n",
    "# loss\n",
    "ax1 = plt.subplot(212)\n",
    "ax1.plot(embedder._history['loss'])\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "\n",
    "#umap_loss\n",
    "ax2 = plt.subplot(221)\n",
    "ax2.plot(embedder._history['umap_loss'])\n",
    "ax2.set_ylabel('umap_loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "\n",
    "# reconstruction loss\n",
    "ax3 = plt.subplot(222)\n",
    "ax3.plot(embedder._history['reconstruction_loss'])\n",
    "ax3.set_ylabel('reconstruction_loss')\n",
    "ax3.set_xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-533091b8c696>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0max1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m212\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_history\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.cla()\n",
    "# loss\n",
    "ax1 = plt.subplot(212)\n",
    "ax1.plot(embedder._history['val_loss'])\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "\n",
    "\n",
    "# reconstruction loss\n",
    "ax3 = plt.subplot(211)\n",
    "ax3.plot(embedder._history['val_reconstruction_loss'])\n",
    "ax3.set_ylabel('val_reconstruction_loss')\n",
    "ax3.set_xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load parametric umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-d4c1c124c4ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mumap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparametric_umap\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_ParametricUMAP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnew_embedder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_ParametricUMAP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./parametric_umap_models/parametric_umap_autoencoder'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Downloads\\anaconda\\envs\\DR2\\lib\\site-packages\\umap\\parametric_umap.py\u001b[0m in \u001b[0;36mload_ParametricUMAP\u001b[1;34m(save_location, verbose)\u001b[0m\n\u001b[0;32m    900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m     \u001b[0mmodel_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"model.pkl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 902\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    903\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pickle of ParametricUMAP model loaded from {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "from umap.parametric_umap import load_ParametricUMAP\n",
    "new_embedder = load_ParametricUMAP('./parametric_umap_models/parametric_umap_autoencoder',verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python xianglinDR2",
   "language": "python",
   "name": "dr2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
